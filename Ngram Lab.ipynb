{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "49a7f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "# if you haven't downloaded punkt before, you only need to run the line below once \n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from nltk.util import bigrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6daa97a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg eBook of Frankenstein, by Mary Wollstonecraft Shelley    This eBook is for the use of anyone anywhere in the United States \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# You will need to leverage the requests package\n",
    "r = requests.get(r'https://www.gutenberg.org/files/84/84-0.txt')\n",
    "Frank_stein = r.text\n",
    "\n",
    "# Remove unwanted new line and tab characters from the text\n",
    "for char in [\"\\n\", \"\\r\", \"\\t\"]:\n",
    "    Frank_stein = Frank_stein.replace(char, \" \")\n",
    "\n",
    "# Check\n",
    "print(Frank_stein[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c435faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the metadata at the beginning - this is slightly different for each book\n",
    "Frank_stein = Frank_stein[1530:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ac77df35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You will rejoice to hear that no disaster has accompanied t\n"
     ]
    }
   ],
   "source": [
    "print(Frank_stein[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ba2c8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creating an N-gram Model\n",
    "# 2 is for bigrams\n",
    "n = 2\n",
    "#specify the text you want to use\n",
    "text = Frank_stein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6c8602d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil', 'forebodings', '.']\n"
     ]
    }
   ],
   "source": [
    "# step 1: tokenize the text into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# step 2: tokenize each sentence into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# step 3: convert each word to lowercase\n",
    "tokenized_text = [[word.lower() for word in sent] for sent in tokenized_sentences]\n",
    "\n",
    "#notice the sentence breaks and what the first 10 items of the tokenized text\n",
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7ea9fbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You will \n"
     ]
    }
   ],
   "source": [
    "#Why tokenize sentences and words? We want to be able to retain sentence boundaries to encode that, too.\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "542288d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we imported this function from nltk\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2a0794f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "# we imported this function from nltk linear models (lm) \n",
    "# it is for Maximum Likelihood Estimation\n",
    "\n",
    "# MLE is the model we will use\n",
    "lm = MLE(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e27ca45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently the vocab length is 0: it has no prior knowledge\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1f1aedfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7872"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model \n",
    "# training data is the bigrams and unigrams \n",
    "# the vocab is all the sentence tokens in the corpus \n",
    "\n",
    "lm.fit(train_data, padded_sents)\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b611f004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('you', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil', 'forebodings', '.')\n"
     ]
    }
   ],
   "source": [
    "# inspect the model's vocabulary. \n",
    "# be sure that a sentence you know exists (from tokenized_text) is in the \n",
    "print(lm.vocab.lookup(tokenized_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5bf3a438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('you', 'will', 'rejoice', 'to', '<UNK>', 'that')\n"
     ]
    }
   ],
   "source": [
    "# see what happens when we include a word that is not in the vocab. \n",
    "print(lm.vocab.lookup('you will rejoice to chocolate that'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "de34b985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.007567613594031058"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many times does 'with' appear in the model?\n",
    "print(lm.counts['with'])\n",
    "\n",
    "# what is the probability of 'with' appearing? \n",
    "# this is technically the relative frequency of daisy appearing \n",
    "lm.score('with')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "69c3668f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the score of 'UNK'? \n",
    "\n",
    "lm.score(\"<UNK>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9eb1b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Does the relative frequency of 'UNK' change your assumption about how the model behaves?\n",
    "\n",
    "##How should we change our model to account for the fact the <UNK> words are not accounted for by the model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2c114f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'and', 'she', 'busied', 'myself', 'and', 'a', 'little', 'chance', ',', 'but', 'it', 'appeared', 'to', 'five', 'persons', ',', 'and', 'i', 'remembered']\n"
     ]
    }
   ],
   "source": [
    "#There is a certain amount of randomness encoded into n-gram models. This prevents a model from becoming entirely deterministic. Maximum Likelihood Estimation without some degree of randomness will only produce the most likely result every time. Setting Random Seed means we will get the same result every time.\n",
    "# generate a 20 word sentence starting with the word, 'I'\n",
    "\n",
    "print(lm.generate(20, text_seed= 'The', random_seed=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6a97fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(lm, num_words, text_seed, random_seed=200):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in lm.generate(num_words, text_seed=text_seed, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6f1fefaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', and she busied myself and a little chance, but it appeared to five persons, and i remembered'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now generate sentences that look much nicer. \n",
    "generate_sent(lm, 20, text_seed='I', random_seed = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a21452d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
